{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b496481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import torch\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import RegexpTokenizer \n",
    "from collections import OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch import optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15523422",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords = False\n",
    "use_lemmatization = False\n",
    "rm_punc = False\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6cb8b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc(sent,\n",
    "                 punc = False,\n",
    "                 lemma=False, \n",
    "                 remove_stopwords=False):\n",
    "    \n",
    "    # a simple tokenizer with case folding and an option to use lemmatization\n",
    "    sent = sent.lower()\n",
    "    #tokens = sent.split()\n",
    "    tokens = tokenizer.tokenize(sent)\n",
    "\n",
    "    if punc:\n",
    "        tokens = [token for token in tokens if not tokenizer.tokenize(token) == []]\n",
    "    if lemma:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "def basic_text_processing(corpus, num_words):\n",
    "    vocab = set()\n",
    "    all_tokens = []\n",
    "    \n",
    "    # tokenization\n",
    "    for doc in tqdm(corpus):\n",
    "        tokens = tokenize_doc(doc, punc=rm_punc, lemma=use_lemmatization, remove_stopwords=remove_stopwords)\n",
    "        vocab.update(set(tokens))\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # TODO START\n",
    "    # We only want to train with the top num_words MOST FREQUENT words\n",
    "    # Output a variable called ``train_tokens\" that is similar to all_tokens\n",
    "    # variable but without infrequent words\n",
    "    freq_words = Counter(all_tokens)\n",
    "    freq_words = sorted(freq_words, key=freq_words.get, reverse=True)\n",
    "    train_tokens = [token for token in all_tokens if token in freq_words[:num_words]]\n",
    "    # TODO END\n",
    "    \n",
    "    # generating vocabulary from the train_tokens\n",
    "    word_counts = Counter(train_tokens)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True) \n",
    "    i2w = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    w2i = {word: ii for ii, word in i2w.items()}\n",
    "    \n",
    "    return  w2i, i2w, train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05b360ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(words, idx, window_size=5):\n",
    "    R = random.randint(1, 5)\n",
    "    start = max(0,idx-R)\n",
    "    end = min(idx+R,len(words)-1)\n",
    "    targets = words[start:idx] + words[idx+1:end+1] # +1 since doesn't include this idx\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a6fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size = 5):\n",
    "    for i in range(0, len(words), batch_size):\n",
    "        curr = words[i:i + batch_size]   # current batch\n",
    "        batch_x, batch_y = [], []\n",
    "        \n",
    "        for ii in range(len(curr)):\n",
    "            x = [curr[ii]]\n",
    "            y = get_targets(curr, ii)\n",
    "            batch_x.extend(x * len(y))\n",
    "            batch_y.extend(y)\n",
    "        \n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82365530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cc_news (/home/nhatvan1561/.cache/huggingface/datasets/cc_news/plain_text/1.0.0/ae469e556251e6e7e20a789f93803c7de19d0c4311b6854ab072fecb4e401bd6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3f683b32eb4a6791ebc0283dcd524a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CC-News dataset contains news articles from news sites all over the world. \n",
    "#The data is available on AWS S3 in the Common Crawl bucket at /crawl-data/CC-NEWS/. \n",
    "#This version of the dataset has been prepared using news-please - an integrated web crawler and information extractor for news.\n",
    "#It contains 708241 English language news articles published between Jan 2017 and December 2019. \n",
    "#It represents a small portion of the English language subset of the CC-News dataset.\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cc_news\")\n",
    "corpus = dataset['train']['text'][:150000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54236747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all configurations go here\n",
    "# TODO\n",
    "# You will need to set configurations below to a suitable values\n",
    "# As for learning rate, the current value should work (but you are welcome to change it)\n",
    "n_vocab = 1000  # maximum size of vocab\n",
    "n_embed = 500 # size of embedding\n",
    "lr = 0.003 # learning rate\n",
    "n_negative_samples = 3 # number negative examples per positive example\n",
    "ws = 5  # window size\n",
    "batch_size =  500 # batch size for sampling positive examples\n",
    "n_epochs =  10 #umber of training epochs\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d89e2023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 150000/150000 [00:21<00:00, 7055.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 1000\n"
     ]
    }
   ],
   "source": [
    "# this cell might take 20 minutes to run, so be patient!\n",
    "# optional: you might want to save these intermediate results to disk\n",
    "# so that next time you open Google Colab, you don't need to\n",
    "# run this again\n",
    "w2i, i2w, train_tokens = basic_text_processing(corpus, num_words=n_vocab)\n",
    "int_words = [w2i[token] for token in train_tokens]\n",
    "print(\"Vocab Size:\", len(w2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ced48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540d1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram(W,\n",
    "                   C,\n",
    "                   int_words,\n",
    "                   n_vocab,\n",
    "                   n_embed,\n",
    "                   learning_rate,\n",
    "                   n_negative_samples,\n",
    "                   batch_size,\n",
    "                   window_size,\n",
    "                   n_epochs,\n",
    "                   print_every=100):\n",
    "    \n",
    "    optimizer = optim.Adam([W, C], lr=learning_rate)    \n",
    "    \n",
    "    W = torch.nn.init.uniform_(W, -0.10, +0.10)\n",
    "    C = torch.nn.init.uniform_(C, -0.10, +0.10)\n",
    "\n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for inputs, targets in get_batches(int_words, batch_size=batch_size, window_size=window_size):\n",
    "            step += 1\n",
    "            \n",
    "            targets_indices = torch.LongTensor(inputs).to(device)\n",
    "            contexts_indices = torch.LongTensor(targets).to(device)\n",
    "\n",
    "            \n",
    "            # retrieve vectors of target words and positive context words\n",
    "            embeded_targets = W[targets_indices]\n",
    "            embeded_pos_contexts = C[contexts_indices]\n",
    "            \n",
    "            batch_size, embed_size = embeded_targets.shape\n",
    "           \n",
    "            # retrieve vectors of negative examples\n",
    "            noise_dist = torch.ones(n_vocab)\n",
    "            noise_words = torch.multinomial(noise_dist, \n",
    "                                            num_samples=batch_size*n_negative_samples, \n",
    "                                            replacement=True)\n",
    "            noise_words = noise_words.to(device)\n",
    "            embed_neg_contexts = C[noise_words].view(batch_size, n_negative_samples, n_embed)\n",
    "            \n",
    "            #print(embeded_targets.shape)\n",
    "            #print(embeded_pos_contexts.shape)\n",
    "            #print(embed_neg_contexts.shape)\n",
    "            # TODO START\n",
    "            # calculating the final loss and output it to ``loss\" variable\n",
    "            embeded_targets = embeded_targets.view(batch_size,embed_size,1)\n",
    "            embeded_pos_contexts = embeded_pos_contexts.view(batch_size,1, embed_size)\n",
    "            \n",
    "            pos_loss = torch.bmm(embeded_pos_contexts, embeded_targets).sigmoid().log().squeeze()\n",
    "            neg_loss = torch.bmm(embed_neg_contexts.neg(), embeded_targets).sigmoid().log().squeeze().sum(1)\n",
    "     \n",
    "            \n",
    "            loss = -(pos_loss + neg_loss).mean()\n",
    "            # loss = \n",
    "            # TODO END\n",
    "            \n",
    "            \n",
    "            # optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (step % print_every) == 0:\n",
    "                print(\"Epoch: {}/{} | Loss: {:.4f}\".format(epoch+1, n_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4747672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of W and C weight matrix\n",
    "W = torch.nn.Parameter(torch.zeros((n_vocab, n_embed), dtype=torch.float32))\n",
    "C = torch.nn.Parameter(torch.zeros((n_vocab, n_embed), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb795023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Loss: 1.6960\n",
      "Epoch: 1/10 | Loss: 1.4639\n",
      "Epoch: 1/10 | Loss: 1.6957\n",
      "Epoch: 1/10 | Loss: 1.5532\n",
      "Epoch: 1/10 | Loss: 1.3995\n",
      "Epoch: 1/10 | Loss: 1.5826\n",
      "Epoch: 1/10 | Loss: 1.4841\n",
      "Epoch: 1/10 | Loss: 1.1790\n",
      "Epoch: 1/10 | Loss: 0.9597\n",
      "Epoch: 1/10 | Loss: 1.1358\n",
      "Epoch: 1/10 | Loss: 1.4859\n",
      "Epoch: 1/10 | Loss: 1.4530\n",
      "Epoch: 1/10 | Loss: 1.4797\n",
      "Epoch: 1/10 | Loss: 1.5411\n",
      "Epoch: 1/10 | Loss: 1.5480\n",
      "Epoch: 1/10 | Loss: 1.5778\n",
      "Epoch: 1/10 | Loss: 1.4586\n",
      "Epoch: 1/10 | Loss: 1.5079\n",
      "Epoch: 1/10 | Loss: 1.6091\n",
      "Epoch: 1/10 | Loss: 1.2274\n",
      "Epoch: 1/10 | Loss: 1.4380\n",
      "Epoch: 1/10 | Loss: 1.5382\n",
      "Epoch: 1/10 | Loss: 1.4753\n",
      "Epoch: 1/10 | Loss: 1.4438\n",
      "Epoch: 1/10 | Loss: 1.2537\n",
      "Epoch: 1/10 | Loss: 1.3487\n",
      "Epoch: 1/10 | Loss: 1.4717\n",
      "Epoch: 1/10 | Loss: 1.3996\n",
      "Epoch: 1/10 | Loss: 1.2346\n",
      "Epoch: 1/10 | Loss: 1.4414\n",
      "Epoch: 1/10 | Loss: 1.3835\n",
      "Epoch: 1/10 | Loss: 1.4895\n",
      "Epoch: 1/10 | Loss: 1.4757\n",
      "Epoch: 1/10 | Loss: 1.4717\n",
      "Epoch: 1/10 | Loss: 1.4320\n",
      "Epoch: 1/10 | Loss: 1.4811\n",
      "Epoch: 1/10 | Loss: 1.3260\n",
      "Epoch: 1/10 | Loss: 1.4349\n",
      "Epoch: 1/10 | Loss: 1.4505\n",
      "Epoch: 1/10 | Loss: 1.4854\n",
      "Epoch: 1/10 | Loss: 1.5226\n",
      "Epoch: 1/10 | Loss: 1.3879\n",
      "Epoch: 1/10 | Loss: 1.4918\n",
      "Epoch: 1/10 | Loss: 1.4611\n",
      "Epoch: 1/10 | Loss: 1.4857\n",
      "Epoch: 1/10 | Loss: 1.4502\n",
      "Epoch: 1/10 | Loss: 1.4354\n",
      "Epoch: 1/10 | Loss: 1.5188\n",
      "Epoch: 1/10 | Loss: 1.3916\n",
      "Epoch: 1/10 | Loss: 1.3972\n",
      "Epoch: 1/10 | Loss: 1.4996\n",
      "Epoch: 1/10 | Loss: 1.4993\n",
      "Epoch: 1/10 | Loss: 1.3473\n",
      "Epoch: 1/10 | Loss: 1.3705\n",
      "Epoch: 1/10 | Loss: 1.3325\n",
      "Epoch: 1/10 | Loss: 1.3809\n",
      "Epoch: 1/10 | Loss: 1.5133\n",
      "Epoch: 1/10 | Loss: 1.3722\n",
      "Epoch: 1/10 | Loss: 1.4038\n",
      "Epoch: 1/10 | Loss: 1.4272\n",
      "Epoch: 1/10 | Loss: 1.4541\n",
      "Epoch: 1/10 | Loss: 1.3788\n",
      "Epoch: 1/10 | Loss: 1.5261\n",
      "Epoch: 1/10 | Loss: 1.4970\n",
      "Epoch: 1/10 | Loss: 1.6622\n",
      "Epoch: 1/10 | Loss: 1.4405\n",
      "Epoch: 1/10 | Loss: 1.5221\n",
      "Epoch: 1/10 | Loss: 1.4744\n",
      "Epoch: 1/10 | Loss: 1.5341\n",
      "Epoch: 1/10 | Loss: 1.3991\n",
      "Epoch: 1/10 | Loss: 1.4778\n",
      "Epoch: 1/10 | Loss: 1.5734\n",
      "Epoch: 1/10 | Loss: 1.4393\n",
      "Epoch: 1/10 | Loss: 1.2179\n",
      "Epoch: 1/10 | Loss: 1.5918\n",
      "Epoch: 1/10 | Loss: 1.3437\n",
      "Epoch: 1/10 | Loss: 1.4618\n",
      "Epoch: 1/10 | Loss: 1.2966\n",
      "Epoch: 1/10 | Loss: 1.2592\n",
      "Epoch: 1/10 | Loss: 1.2465\n",
      "Epoch: 1/10 | Loss: 1.4942\n",
      "Epoch: 1/10 | Loss: 1.3146\n",
      "Epoch: 1/10 | Loss: 1.4755\n",
      "Epoch: 1/10 | Loss: 1.5899\n",
      "Epoch: 1/10 | Loss: 1.4289\n",
      "Epoch: 1/10 | Loss: 1.3539\n",
      "Epoch: 1/10 | Loss: 1.6124\n",
      "Epoch: 1/10 | Loss: 1.4879\n",
      "Epoch: 1/10 | Loss: 1.4383\n",
      "Epoch: 1/10 | Loss: 1.3142\n",
      "Epoch: 1/10 | Loss: 1.4364\n",
      "Epoch: 1/10 | Loss: 1.4634\n",
      "Epoch: 1/10 | Loss: 1.5221\n",
      "Epoch: 1/10 | Loss: 1.3047\n",
      "Epoch: 1/10 | Loss: 1.4908\n",
      "Epoch: 1/10 | Loss: 1.2627\n",
      "Epoch: 1/10 | Loss: 1.5075\n",
      "Epoch: 1/10 | Loss: 1.4715\n",
      "Epoch: 1/10 | Loss: 1.5566\n",
      "Epoch: 1/10 | Loss: 1.6183\n",
      "Epoch: 1/10 | Loss: 1.5099\n",
      "Epoch: 1/10 | Loss: 1.4122\n",
      "Epoch: 1/10 | Loss: 1.3733\n",
      "Epoch: 1/10 | Loss: 1.5528\n",
      "Epoch: 1/10 | Loss: 1.4391\n",
      "Epoch: 1/10 | Loss: 1.2986\n",
      "Epoch: 1/10 | Loss: 1.3569\n",
      "Epoch: 1/10 | Loss: 1.3980\n",
      "Epoch: 1/10 | Loss: 1.2831\n",
      "Epoch: 1/10 | Loss: 1.5824\n",
      "Epoch: 1/10 | Loss: 0.2037\n",
      "Epoch: 1/10 | Loss: 1.4615\n",
      "Epoch: 1/10 | Loss: 1.5548\n",
      "Epoch: 1/10 | Loss: 1.4559\n",
      "Epoch: 1/10 | Loss: 1.3540\n",
      "Epoch: 1/10 | Loss: 1.2429\n",
      "Epoch: 1/10 | Loss: 0.9507\n",
      "Epoch: 1/10 | Loss: 1.0567\n",
      "Epoch: 1/10 | Loss: 1.2744\n",
      "Epoch: 1/10 | Loss: 1.3149\n",
      "Epoch: 1/10 | Loss: 1.0331\n",
      "Epoch: 1/10 | Loss: 1.1105\n",
      "Epoch: 1/10 | Loss: 1.3695\n",
      "Epoch: 1/10 | Loss: 1.3217\n",
      "Epoch: 1/10 | Loss: 1.0727\n",
      "Epoch: 1/10 | Loss: 1.2159\n",
      "Epoch: 1/10 | Loss: 1.1170\n",
      "Epoch: 1/10 | Loss: 1.0855\n",
      "Epoch: 1/10 | Loss: 1.4993\n",
      "Epoch: 1/10 | Loss: 1.4477\n",
      "Epoch: 1/10 | Loss: 1.4359\n",
      "Epoch: 1/10 | Loss: 1.3405\n",
      "Epoch: 1/10 | Loss: 1.3474\n",
      "Epoch: 1/10 | Loss: 1.4951\n",
      "Epoch: 1/10 | Loss: 1.4628\n",
      "Epoch: 1/10 | Loss: 1.4643\n",
      "Epoch: 1/10 | Loss: 1.5123\n",
      "Epoch: 1/10 | Loss: 1.3726\n",
      "Epoch: 1/10 | Loss: 1.4335\n",
      "Epoch: 1/10 | Loss: 1.3506\n",
      "Epoch: 1/10 | Loss: 1.3512\n",
      "Epoch: 1/10 | Loss: 1.3919\n",
      "Epoch: 1/10 | Loss: 1.3221\n",
      "Epoch: 1/10 | Loss: 1.3162\n",
      "Epoch: 1/10 | Loss: 1.6084\n",
      "Epoch: 1/10 | Loss: 1.4170\n",
      "Epoch: 1/10 | Loss: 1.5520\n",
      "Epoch: 1/10 | Loss: 1.4549\n",
      "Epoch: 1/10 | Loss: 1.4305\n",
      "Epoch: 1/10 | Loss: 1.5758\n",
      "Epoch: 1/10 | Loss: 1.3371\n",
      "Epoch: 1/10 | Loss: 1.4160\n",
      "Epoch: 1/10 | Loss: 1.4582\n",
      "Epoch: 1/10 | Loss: 1.3183\n",
      "Epoch: 1/10 | Loss: 1.5191\n",
      "Epoch: 1/10 | Loss: 1.4047\n",
      "Epoch: 1/10 | Loss: 1.4522\n",
      "Epoch: 1/10 | Loss: 1.3323\n",
      "Epoch: 1/10 | Loss: 1.4830\n",
      "Epoch: 1/10 | Loss: 1.3554\n",
      "Epoch: 1/10 | Loss: 1.2790\n",
      "Epoch: 1/10 | Loss: 1.4075\n",
      "Epoch: 1/10 | Loss: 1.3815\n",
      "Epoch: 1/10 | Loss: 1.2378\n",
      "Epoch: 1/10 | Loss: 1.4645\n",
      "Epoch: 1/10 | Loss: 1.3712\n",
      "Epoch: 1/10 | Loss: 1.6384\n",
      "Epoch: 1/10 | Loss: 1.4836\n",
      "Epoch: 1/10 | Loss: 1.4449\n",
      "Epoch: 1/10 | Loss: 1.4814\n",
      "Epoch: 1/10 | Loss: 1.4644\n",
      "Epoch: 1/10 | Loss: 1.3656\n",
      "Epoch: 1/10 | Loss: 1.4490\n",
      "Epoch: 1/10 | Loss: 1.4310\n",
      "Epoch: 1/10 | Loss: 1.3685\n",
      "Epoch: 1/10 | Loss: 1.4900\n",
      "Epoch: 1/10 | Loss: 1.4130\n",
      "Epoch: 1/10 | Loss: 1.4068\n",
      "Epoch: 1/10 | Loss: 1.4542\n",
      "Epoch: 1/10 | Loss: 1.5090\n",
      "Epoch: 1/10 | Loss: 1.3259\n",
      "Epoch: 1/10 | Loss: 1.3069\n",
      "Epoch: 1/10 | Loss: 1.3334\n",
      "Epoch: 1/10 | Loss: 1.4371\n",
      "Epoch: 1/10 | Loss: 1.4631\n",
      "Epoch: 1/10 | Loss: 1.3298\n",
      "Epoch: 1/10 | Loss: 1.3675\n",
      "Epoch: 1/10 | Loss: 1.4194\n",
      "Epoch: 1/10 | Loss: 1.5402\n",
      "Epoch: 1/10 | Loss: 1.4034\n",
      "Epoch: 1/10 | Loss: 1.5084\n",
      "Epoch: 1/10 | Loss: 1.3961\n",
      "Epoch: 1/10 | Loss: 1.4822\n",
      "Epoch: 1/10 | Loss: 1.4321\n",
      "Epoch: 1/10 | Loss: 1.4181\n",
      "Epoch: 1/10 | Loss: 1.4674\n",
      "Epoch: 1/10 | Loss: 1.4391\n",
      "Epoch: 1/10 | Loss: 1.4160\n",
      "Epoch: 1/10 | Loss: 1.3774\n",
      "Epoch: 1/10 | Loss: 1.5261\n",
      "Epoch: 1/10 | Loss: 1.3972\n",
      "Epoch: 1/10 | Loss: 1.4153\n",
      "Epoch: 1/10 | Loss: 1.4239\n",
      "Epoch: 1/10 | Loss: 1.4125\n",
      "Epoch: 1/10 | Loss: 1.4424\n",
      "Epoch: 1/10 | Loss: 1.4427\n",
      "Epoch: 1/10 | Loss: 1.4094\n",
      "Epoch: 1/10 | Loss: 1.3821\n",
      "Epoch: 1/10 | Loss: 1.6231\n",
      "Epoch: 1/10 | Loss: 1.1999\n",
      "Epoch: 1/10 | Loss: 1.4073\n",
      "Epoch: 1/10 | Loss: 1.5808\n",
      "Epoch: 1/10 | Loss: 1.4084\n",
      "Epoch: 1/10 | Loss: 1.2777\n",
      "Epoch: 1/10 | Loss: 1.3659\n",
      "Epoch: 1/10 | Loss: 1.6995\n",
      "Epoch: 1/10 | Loss: 1.4689\n",
      "Epoch: 1/10 | Loss: 1.4660\n",
      "Epoch: 1/10 | Loss: 1.3359\n",
      "Epoch: 1/10 | Loss: 1.4631\n",
      "Epoch: 1/10 | Loss: 1.5195\n",
      "Epoch: 1/10 | Loss: 1.4731\n",
      "Epoch: 1/10 | Loss: 1.3446\n",
      "Epoch: 1/10 | Loss: 1.3517\n",
      "Epoch: 1/10 | Loss: 1.4190\n",
      "Epoch: 1/10 | Loss: 1.5048\n",
      "Epoch: 1/10 | Loss: 1.4174\n",
      "Epoch: 1/10 | Loss: 1.5983\n",
      "Epoch: 1/10 | Loss: 1.3256\n",
      "Epoch: 1/10 | Loss: 1.4233\n",
      "Epoch: 1/10 | Loss: 1.3703\n",
      "Epoch: 1/10 | Loss: 1.4785\n",
      "Epoch: 1/10 | Loss: 1.3766\n",
      "Epoch: 1/10 | Loss: 1.3606\n",
      "Epoch: 1/10 | Loss: 1.3934\n",
      "Epoch: 1/10 | Loss: 1.5666\n",
      "Epoch: 1/10 | Loss: 1.3899\n",
      "Epoch: 1/10 | Loss: 1.3681\n",
      "Epoch: 1/10 | Loss: 1.4430\n",
      "Epoch: 1/10 | Loss: 1.2979\n",
      "Epoch: 1/10 | Loss: 1.4473\n",
      "Epoch: 1/10 | Loss: 1.5737\n",
      "Epoch: 1/10 | Loss: 1.5212\n",
      "Epoch: 1/10 | Loss: 1.5529\n",
      "Epoch: 1/10 | Loss: 1.4220\n",
      "Epoch: 1/10 | Loss: 1.3950\n",
      "Epoch: 1/10 | Loss: 1.4581\n",
      "Epoch: 1/10 | Loss: 1.4090\n",
      "Epoch: 1/10 | Loss: 1.4342\n",
      "Epoch: 1/10 | Loss: 1.3677\n",
      "Epoch: 1/10 | Loss: 1.4959\n",
      "Epoch: 1/10 | Loss: 1.3669\n",
      "Epoch: 1/10 | Loss: 1.3898\n"
     ]
    }
   ],
   "source": [
    "train_skipgram(W,\n",
    "               C,\n",
    "               int_words,\n",
    "               n_vocab=n_vocab,\n",
    "               n_embed=n_embed,\n",
    "               learning_rate=lr,\n",
    "               n_negative_samples=n_negative_samples,\n",
    "               batch_size=batch_size,\n",
    "               window_size=ws,\n",
    "               n_epochs=n_epochs,\n",
    "               print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac96ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final embeddings is the summation of the two matrix (check lecture slides)\n",
    "embeddings = W.data.to('cpu').data.numpy()\n",
    "embeddings += C.data.to('cpu').data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d8a91",
   "metadata": {},
   "source": [
    "### Evaluation via Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c995ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_viz_words = 100\n",
    "tsne = TSNE()\n",
    "embeddings_tsne = tsne.fit_transform(embeddings[:n_viz_words, :])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "for i in range(n_viz_words):\n",
    "    plt.scatter(*embeddings_tsne[i, :], color = 'red', s=40)\n",
    "    plt.annotate(i2w[i], (embeddings_tsne[i, 0], embeddings_tsne[i, 1]), alpha = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0a899",
   "metadata": {},
   "source": [
    "### Evaluation via Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(query, w2i, embeddings, strategy):\n",
    "    # TODO\n",
    "    # input: query or document, vocabulary w2i\n",
    "    # input: trained word2vec embeddings\n",
    "    # input: strategy: either 'average' or 'concatenate'\n",
    "    # output: vector representation of the document query\n",
    "    w2v_query = tokenize_doc(query, punc=rm_punc, lemma=use_lemmatization, remove_stopwords=remove_stopwords)\n",
    "    idx = [w2i[word] for word in w2v_query]\n",
    "    v = [embeddings[i] for i in idx]\n",
    "    \n",
    "    assert strategy in ['average', 'concatenate']\n",
    "    \n",
    "    # get vectors of each word in the query\n",
    "    \n",
    "    # sentence aggregation strategy\n",
    "    if strategy == 'average':\n",
    "        # TODO START\n",
    "        vector = [em.mean() for em in v]\n",
    "        # vector = \n",
    "        # TODO END\n",
    "    else:\n",
    "        # TODO START\n",
    "        vector = [em for em in v]\n",
    "        # TODO END\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04afb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test document similarity\n",
    "q = transform('today I am very happy', w2i, embeddings, strategy='average')\n",
    "v = transform('today I feel so fantastic', w2i, embeddings, strategy='average')\n",
    "sim = np.dot(q, v)/(np.linalg.norm(q)* np.linalg.norm(v))\n",
    "print(\"Cosine Similarity: {}\".format(sim)) # this score should be high / close to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(strategy):\n",
    "    df = pd.read_csv('./quora_train.csv')\n",
    "    print(\"Loaded {} pairs\".format(len(df)))\n",
    "    pairs = list(zip(df['question1'].astype(str), df['question2'].astype(str)))\n",
    "    \n",
    "    all_sims = []\n",
    "    \n",
    "    for doc1, doc2 in tqdm(pairs):\n",
    "        q = transform(doc1, w2i, embeddings, strategy=strategy)\n",
    "        v = transform(doc2, w2i, embeddings, strategy=strategy)\n",
    "        \n",
    "        diff = len(q) - len(v)\n",
    "        if diff > 0:\n",
    "            v = np.pad(v, (0, np.abs(diff)))\n",
    "        else:\n",
    "            q = np.pad(q, (0, np.abs(diff)))\n",
    "        \n",
    "        sim = np.dot(q, v)/(np.linalg.norm(q)* np.linalg.norm(v))\n",
    "        all_sims.append(sim)\n",
    "        \n",
    "    return np.mean(all_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf214d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_similarity = evaluation('average')\n",
    "print(\"Final Average Similarity using Average Strategy: {}\".format(avg_similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_similarity = evaluation('concatenate')\n",
    "print(\"Final Average Similarity using Concatenation Strategy: {}\".format(avg_similarity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0718306951c543f9ab1700229e5c3e5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9dbd4eaf2f26468a91a88fd22327460f",
       "style": "IPY_MODEL_375bd16927644612842cf2215d231b6b",
       "value": " 1/1 [00:00&lt;00:00,  1.83it/s]"
      }
     },
     "29e273c83d6d4107849fb5ac592ed40c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2e6d280046ae4371bc8e4f3519c4c142": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "375bd16927644612842cf2215d231b6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "539046ed413540b8929a07b234771d6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8d857fdcc2e3494b94923334716c0336",
       "style": "IPY_MODEL_2e6d280046ae4371bc8e4f3519c4c142",
       "value": "100%"
      }
     },
     "5b08beeb860c43328729559b1394f2d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7b3f683b32eb4a6791ebc0283dcd524a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_539046ed413540b8929a07b234771d6b",
        "IPY_MODEL_99e74ce0b94a497f971d09f794564e6f",
        "IPY_MODEL_0718306951c543f9ab1700229e5c3e5d"
       ],
       "layout": "IPY_MODEL_5b08beeb860c43328729559b1394f2d8"
      }
     },
     "8d857fdcc2e3494b94923334716c0336": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "99e74ce0b94a497f971d09f794564e6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_cab5c9b4fd08470cb51b1bd2dbda7e23",
       "max": 1,
       "style": "IPY_MODEL_29e273c83d6d4107849fb5ac592ed40c",
       "value": 1
      }
     },
     "9dbd4eaf2f26468a91a88fd22327460f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cab5c9b4fd08470cb51b1bd2dbda7e23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
