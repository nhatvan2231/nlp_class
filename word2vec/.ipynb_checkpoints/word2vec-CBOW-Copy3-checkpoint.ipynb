{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b496481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import torch\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import RegexpTokenizer \n",
    "from collections import OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15523422",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords = True\n",
    "use_lemmatization = True\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "catchedStopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6cb8b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc(sent,\n",
    "                 lemma=False, \n",
    "                 remove_stopwords=False):\n",
    "    \n",
    "    # a simple tokenizer with case folding and an option to use lemmatization\n",
    "    sent = sent.lower()\n",
    "    tokens = sent.split()\n",
    "    tokens = [*Counter(tokens).keys()]\n",
    "    \n",
    "    if lemma:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in catchedStopWords]\n",
    "    return tokens\n",
    "\n",
    "def basic_text_processing(corpus, num_words):\n",
    "    vocab = set()\n",
    "    all_tokens = []\n",
    "    \n",
    "    # tokenization\n",
    "    for doc in tqdm(corpus):\n",
    "        tokens = tokenize_doc(doc, lemma=use_lemmatization, remove_stopwords=remove_stopwords)\n",
    "        vocab.update(set(tokens))\n",
    "        all_tokens.extend(tokens)\n",
    "    print(\"Tokenization complete\")\n",
    "    # TODO START\n",
    "    # We only want to train with the top num_words MOST FREQUENT words\n",
    "    # Output a variable called ``train_tokens\" that is similar to all_tokens\n",
    "    # variable but without infrequent words\n",
    "    freq_words = dict(Counter(all_tokens).most_common(num_words))\n",
    "    train_tokens = [token for token in all_tokens if token in freq_words]\n",
    "\n",
    "    # TODO END\n",
    "    \n",
    "    # generating vocabulary from the train_tokens\n",
    "    word_counts = Counter(train_tokens)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True) \n",
    "    i2w = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    w2i = {word: ii for ii, word in i2w.items()}\n",
    "    \n",
    "    return  w2i, i2w, train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05b360ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(words, idx):\n",
    "    contexts = [words[idx-2], words[idx-1], words[idx+1], words[idx+2]]\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a6fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size):  #, window_size = 4):\n",
    "    for i in range(0, len(words), batch_size):\n",
    "        curr = words[i:i + batch_size]   # current batch\n",
    "        batch_x, batch_y = [], []\n",
    "        #batch = []\n",
    "        for ii in range(2,len(curr)-2):\n",
    "            x = get_contexts(curr, ii)\n",
    "            y = curr[ii]\n",
    "            #batch.append((x,y))\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "        \n",
    "        yield batch_x, batch_y\n",
    "        #yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82365530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cc_news (/home/nhatvan1561/.cache/huggingface/datasets/cc_news/plain_text/1.0.0/ae469e556251e6e7e20a789f93803c7de19d0c4311b6854ab072fecb4e401bd6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee40cf600d541159442752eb8ba67a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CC-e woNews dataset contains news articles from news sites all over thrld. \n",
    "#The data is available on AWS S3 in the Common Crawl bucket at /crawl-data/CC-NEWS/. \n",
    "#This version of the dataset has been prepared using news-please - an integrated web crawler and information extractor for news.\n",
    "#It contains 708241 English language news articles published between Jan 2017 and December 2019. \n",
    "#It represents a small portion of the English language subset of the CC-News dataset.\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cc_news\")\n",
    "corpus = dataset['train']['text'][:150000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54236747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all configurations go here\n",
    "# TODO\n",
    "# You will need to set configurations below to a suitable values\n",
    "# As for learning rate, the current value should work (but you are welcome to change it)\n",
    "n_vocab = 10000  # maximum size of vocab\n",
    "n_embed = 200 # size of embedding\n",
    "lr = 0.001 # learning rate\n",
    "ws = 5  # window size\n",
    "batch_size =  50 # batch size for sampling positive examples\n",
    "n_epochs =  5 #umber of training epochs\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d89e2023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 150000/150000 [02:03<00:00, 1214.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete\n",
      "Vocab Size: 10000\n"
     ]
    }
   ],
   "source": [
    "# this cell might take 20 minutes to run, so be patient!\n",
    "# optional: you might want to save these intermediate results to disk\n",
    "# so that next time you open Google Colab, you don't need to\n",
    "# run this again\n",
    "w2i, i2w, train_tokens = basic_text_processing(corpus, num_words=n_vocab)\n",
    "int_words = [w2i[token] for token in train_tokens]\n",
    "print(\"Vocab Size:\", len(w2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ced48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "540d1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cbow(    w1,\n",
    "                   w2,\n",
    "                   int_words,\n",
    "                   n_vocab,\n",
    "                   n_embed,\n",
    "                   learning_rate,\n",
    "                   batch_size,\n",
    "                   n_epochs,\n",
    "                   print_every=100):\n",
    "    \n",
    "    optimizer = optim.Adam([w1, w2], lr=learning_rate)    \n",
    "    w1 = torch.nn.init.uniform_(w1, -0.10, +0.10)\n",
    "    w2 = torch.nn.init.uniform_(w2, -0.10, +0.10)\n",
    "\n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for inputs, targets in get_batches(int_words, batch_size=batch_size):\n",
    "            step += 1\n",
    "            #stime = time.time()\n",
    "\n",
    "            contexts_indices = torch.LongTensor(inputs).to(device) # number_batches x window_size\n",
    "            targets_indices = torch.LongTensor(targets).to(device) # number_batches x label\n",
    "            b_size = contexts_indices.shape[0]\n",
    "           \n",
    "            \n",
    "            # retrieve vectors of target words and positive context words\n",
    "            W1 = w1[contexts_indices].mean(1).view(b_size, n_embed,1)\n",
    "            W2 = w2.expand(b_size,-1,-1)\n",
    "            o_layer = torch.bmm(W2, W1)\n",
    "            nom = o_layer[torch.arange(b_size),targets_indices]\n",
    "            den = o_layer.exp().sum(1).log()\n",
    "            loss = (-nom + den).mean()\n",
    "            #print(time.time()-stime)\n",
    "\n",
    "            # optimization\n",
    "            optimizer.zero_grad()\n",
    "            #print(loss)\n",
    "            loss.backward()\n",
    "            #print(time.time()-stime)\n",
    "            #return #loss, w1.grad, w2.grad, o_layer.grad\n",
    "            optimizer.step()\n",
    "            #print(time.time()-stime)\n",
    "            if (step % print_every) == 0:\n",
    "                #print(time.time()-stime)\n",
    "                print(\"Epoch: {}/{} | Loss: {:.4f}\".format(epoch+1, n_epochs, loss.item()))\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4747672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of W and C weight matrix\n",
    "w1 = torch.nn.Parameter(torch.zeros((n_vocab, n_embed), dtype=torch.float32))\n",
    "w2 = torch.nn.Parameter(torch.zeros((n_vocab, n_embed), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb795023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 | Loss: 9.2044\n",
      "Epoch: 1/5 | Loss: 9.1870\n",
      "Epoch: 1/5 | Loss: 8.8521\n",
      "Epoch: 1/5 | Loss: 8.8885\n",
      "Epoch: 1/5 | Loss: 8.6991\n",
      "Epoch: 1/5 | Loss: 8.8186\n",
      "Epoch: 1/5 | Loss: 8.6850\n",
      "Epoch: 1/5 | Loss: 8.7848\n",
      "Epoch: 1/5 | Loss: 8.4063\n",
      "Epoch: 1/5 | Loss: 8.6970\n",
      "Epoch: 1/5 | Loss: 8.9250\n",
      "Epoch: 1/5 | Loss: 9.1246\n",
      "Epoch: 1/5 | Loss: 9.0343\n",
      "Epoch: 1/5 | Loss: 9.1154\n",
      "Epoch: 1/5 | Loss: 8.4627\n",
      "Epoch: 1/5 | Loss: 9.1453\n",
      "Epoch: 1/5 | Loss: 8.7728\n",
      "Epoch: 1/5 | Loss: 8.5189\n",
      "Epoch: 1/5 | Loss: 8.7222\n",
      "Epoch: 1/5 | Loss: 9.0183\n",
      "Epoch: 1/5 | Loss: 8.2685\n",
      "Epoch: 1/5 | Loss: 8.5440\n",
      "Epoch: 1/5 | Loss: 8.3075\n",
      "Epoch: 1/5 | Loss: 8.3372\n",
      "Epoch: 1/5 | Loss: 8.6932\n",
      "Epoch: 1/5 | Loss: 8.0035\n",
      "Epoch: 1/5 | Loss: 7.9173\n",
      "Epoch: 1/5 | Loss: 8.0518\n",
      "Epoch: 1/5 | Loss: 8.5049\n",
      "Epoch: 1/5 | Loss: 7.2272\n",
      "Epoch: 1/5 | Loss: 8.8028\n",
      "Epoch: 1/5 | Loss: 8.7321\n",
      "Epoch: 1/5 | Loss: 8.5675\n",
      "Epoch: 1/5 | Loss: 8.6277\n",
      "Epoch: 1/5 | Loss: 8.1945\n",
      "Epoch: 1/5 | Loss: 9.0307\n",
      "Epoch: 1/5 | Loss: 7.9701\n",
      "Epoch: 1/5 | Loss: 8.0633\n",
      "Epoch: 1/5 | Loss: 8.8085\n",
      "Epoch: 1/5 | Loss: 8.4915\n",
      "Epoch: 1/5 | Loss: 7.8094\n",
      "Epoch: 1/5 | Loss: 8.8007\n",
      "Epoch: 1/5 | Loss: 8.7063\n",
      "Epoch: 1/5 | Loss: 8.4471\n",
      "Epoch: 1/5 | Loss: 6.6797\n",
      "Epoch: 1/5 | Loss: 8.3574\n",
      "Epoch: 1/5 | Loss: 7.5036\n",
      "Epoch: 1/5 | Loss: 8.8792\n",
      "Epoch: 1/5 | Loss: 6.9340\n",
      "Epoch: 1/5 | Loss: 9.0912\n",
      "Epoch: 1/5 | Loss: 8.3550\n",
      "Epoch: 1/5 | Loss: 7.1624\n",
      "Epoch: 1/5 | Loss: 8.6672\n",
      "Epoch: 1/5 | Loss: 7.3577\n",
      "Epoch: 1/5 | Loss: 7.9121\n",
      "Epoch: 1/5 | Loss: 1.4523\n",
      "Epoch: 1/5 | Loss: 8.1484\n",
      "Epoch: 1/5 | Loss: 8.1298\n",
      "Epoch: 1/5 | Loss: 7.6963\n",
      "Epoch: 1/5 | Loss: 8.3001\n",
      "Epoch: 1/5 | Loss: 1.1102\n",
      "Epoch: 1/5 | Loss: 8.0782\n",
      "Epoch: 1/5 | Loss: 8.0167\n",
      "Epoch: 1/5 | Loss: 8.8303\n",
      "Epoch: 1/5 | Loss: 1.1121\n",
      "Epoch: 1/5 | Loss: 8.1015\n",
      "Epoch: 1/5 | Loss: 8.3482\n",
      "Epoch: 1/5 | Loss: 8.5871\n",
      "Epoch: 1/5 | Loss: 8.1978\n",
      "Epoch: 1/5 | Loss: 8.6374\n",
      "Epoch: 1/5 | Loss: 8.7108\n",
      "Epoch: 1/5 | Loss: 8.2925\n",
      "Epoch: 1/5 | Loss: 5.1970\n",
      "Epoch: 1/5 | Loss: 8.0997\n",
      "Epoch: 1/5 | Loss: 7.9167\n",
      "Epoch: 1/5 | Loss: 6.8218\n",
      "Epoch: 1/5 | Loss: 1.5280\n",
      "Epoch: 1/5 | Loss: 8.4622\n",
      "Epoch: 1/5 | Loss: 8.3140\n",
      "Epoch: 1/5 | Loss: 6.5999\n",
      "Epoch: 1/5 | Loss: 7.1202\n",
      "Epoch: 1/5 | Loss: 8.6024\n",
      "Epoch: 1/5 | Loss: 8.1120\n",
      "Epoch: 1/5 | Loss: 7.7862\n",
      "Epoch: 1/5 | Loss: 7.6270\n",
      "Epoch: 1/5 | Loss: 8.3798\n",
      "Epoch: 1/5 | Loss: 7.9452\n",
      "Epoch: 1/5 | Loss: 8.4205\n",
      "Epoch: 1/5 | Loss: 8.6760\n",
      "Epoch: 1/5 | Loss: 8.0247\n",
      "Epoch: 1/5 | Loss: 8.5581\n",
      "Epoch: 1/5 | Loss: 8.0383\n",
      "Epoch: 1/5 | Loss: 8.5094\n",
      "Epoch: 1/5 | Loss: 8.4572\n",
      "Epoch: 1/5 | Loss: 8.6828\n",
      "Epoch: 1/5 | Loss: 8.5214\n",
      "Epoch: 1/5 | Loss: 7.8277\n",
      "Epoch: 1/5 | Loss: 8.1949\n",
      "Epoch: 1/5 | Loss: 7.8434\n",
      "Epoch: 1/5 | Loss: 8.6502\n",
      "Epoch: 1/5 | Loss: 8.2907\n",
      "Epoch: 1/5 | Loss: 8.4576\n",
      "Epoch: 1/5 | Loss: 8.1356\n",
      "Epoch: 1/5 | Loss: 8.0347\n",
      "Epoch: 1/5 | Loss: 9.0647\n",
      "Epoch: 1/5 | Loss: 8.0296\n",
      "Epoch: 1/5 | Loss: 8.2768\n",
      "Epoch: 1/5 | Loss: 7.8375\n",
      "Epoch: 1/5 | Loss: 8.5705\n",
      "Epoch: 1/5 | Loss: 8.7544\n",
      "Epoch: 1/5 | Loss: 7.8131\n",
      "Epoch: 1/5 | Loss: 8.1841\n",
      "Epoch: 1/5 | Loss: 7.4345\n",
      "Epoch: 1/5 | Loss: 7.7944\n",
      "Epoch: 1/5 | Loss: 7.8779\n",
      "Epoch: 1/5 | Loss: 7.8042\n",
      "Epoch: 1/5 | Loss: 8.0417\n",
      "Epoch: 1/5 | Loss: 7.9647\n",
      "Epoch: 1/5 | Loss: 8.2512\n",
      "Epoch: 1/5 | Loss: 8.4177\n",
      "Epoch: 1/5 | Loss: 6.6339\n",
      "Epoch: 1/5 | Loss: 7.1192\n",
      "Epoch: 1/5 | Loss: 7.2814\n",
      "Epoch: 1/5 | Loss: 6.8071\n",
      "Epoch: 1/5 | Loss: 7.6709\n",
      "Epoch: 1/5 | Loss: 8.0130\n",
      "Epoch: 1/5 | Loss: 8.4046\n",
      "Epoch: 1/5 | Loss: 5.8569\n",
      "Epoch: 1/5 | Loss: 8.1735\n",
      "Epoch: 1/5 | Loss: 5.5273\n",
      "Epoch: 1/5 | Loss: 8.0836\n",
      "Epoch: 1/5 | Loss: 5.1854\n",
      "Epoch: 1/5 | Loss: 8.0533\n",
      "Epoch: 1/5 | Loss: 8.5816\n",
      "Epoch: 1/5 | Loss: 7.6771\n",
      "Epoch: 1/5 | Loss: 1.6858\n",
      "Epoch: 1/5 | Loss: 8.4882\n",
      "Epoch: 1/5 | Loss: 8.4724\n",
      "Epoch: 1/5 | Loss: 0.7665\n",
      "Epoch: 1/5 | Loss: 0.3580\n",
      "Epoch: 1/5 | Loss: 0.1153\n",
      "Epoch: 1/5 | Loss: 7.8498\n",
      "Epoch: 1/5 | Loss: 7.5838\n",
      "Epoch: 1/5 | Loss: 8.5539\n",
      "Epoch: 1/5 | Loss: 8.7707\n",
      "Epoch: 1/5 | Loss: 8.0680\n",
      "Epoch: 1/5 | Loss: 8.1757\n",
      "Epoch: 1/5 | Loss: 8.1842\n",
      "Epoch: 1/5 | Loss: 8.1618\n",
      "Epoch: 1/5 | Loss: 7.9814\n",
      "Epoch: 1/5 | Loss: 6.7089\n",
      "Epoch: 1/5 | Loss: 7.5073\n",
      "Epoch: 1/5 | Loss: 7.7434\n",
      "Epoch: 1/5 | Loss: 8.0680\n",
      "Epoch: 1/5 | Loss: 7.8965\n",
      "Epoch: 1/5 | Loss: 8.6840\n",
      "Epoch: 1/5 | Loss: 6.7290\n",
      "Epoch: 1/5 | Loss: 7.7923\n",
      "Epoch: 1/5 | Loss: 7.5146\n",
      "Epoch: 1/5 | Loss: 8.1675\n",
      "Epoch: 1/5 | Loss: 7.6610\n",
      "Epoch: 1/5 | Loss: 8.4897\n",
      "Epoch: 1/5 | Loss: 8.6403\n",
      "Epoch: 1/5 | Loss: 8.3562\n",
      "Epoch: 1/5 | Loss: 8.7521\n",
      "Epoch: 1/5 | Loss: 7.4795\n",
      "Epoch: 1/5 | Loss: 8.0867\n",
      "Epoch: 1/5 | Loss: 7.9370\n",
      "Epoch: 1/5 | Loss: 8.1395\n",
      "Epoch: 1/5 | Loss: 8.0207\n",
      "Epoch: 1/5 | Loss: 7.8456\n",
      "Epoch: 1/5 | Loss: 7.4778\n",
      "Epoch: 1/5 | Loss: 7.4851\n",
      "Epoch: 1/5 | Loss: 8.2825\n",
      "Epoch: 1/5 | Loss: 7.0719\n",
      "Epoch: 1/5 | Loss: 7.7797\n",
      "Epoch: 1/5 | Loss: 8.2230\n",
      "Epoch: 1/5 | Loss: 8.0537\n",
      "Epoch: 1/5 | Loss: 7.3984\n",
      "Epoch: 1/5 | Loss: 8.4822\n",
      "Epoch: 1/5 | Loss: 8.1825\n",
      "Epoch: 1/5 | Loss: 7.9661\n",
      "Epoch: 1/5 | Loss: 8.1718\n",
      "Epoch: 1/5 | Loss: 7.9179\n",
      "Epoch: 1/5 | Loss: 8.5543\n",
      "Epoch: 1/5 | Loss: 7.9325\n",
      "Epoch: 1/5 | Loss: 6.9542\n",
      "Epoch: 1/5 | Loss: 7.4108\n",
      "Epoch: 1/5 | Loss: 7.7707\n",
      "Epoch: 1/5 | Loss: 8.1206\n",
      "Epoch: 1/5 | Loss: 7.4143\n",
      "Epoch: 1/5 | Loss: 7.9743\n",
      "Epoch: 1/5 | Loss: 6.6283\n",
      "Epoch: 1/5 | Loss: 7.5238\n",
      "Epoch: 1/5 | Loss: 7.5151\n",
      "Epoch: 1/5 | Loss: 8.4048\n",
      "Epoch: 1/5 | Loss: 8.3142\n",
      "Epoch: 1/5 | Loss: 7.6862\n",
      "Epoch: 1/5 | Loss: 8.3408\n",
      "Epoch: 1/5 | Loss: 7.6564\n",
      "Epoch: 1/5 | Loss: 8.3216\n",
      "Epoch: 1/5 | Loss: 7.5448\n",
      "Epoch: 1/5 | Loss: 8.7650\n",
      "Epoch: 1/5 | Loss: 7.8907\n",
      "Epoch: 1/5 | Loss: 8.4264\n",
      "Epoch: 1/5 | Loss: 6.9491\n",
      "Epoch: 1/5 | Loss: 6.8587\n",
      "Epoch: 1/5 | Loss: 7.8244\n",
      "Epoch: 1/5 | Loss: 7.7473\n",
      "Epoch: 1/5 | Loss: 6.6552\n",
      "Epoch: 1/5 | Loss: 8.4933\n",
      "Epoch: 1/5 | Loss: 7.8456\n",
      "Epoch: 1/5 | Loss: 8.1448\n",
      "Epoch: 1/5 | Loss: 7.7863\n",
      "Epoch: 1/5 | Loss: 7.3976\n",
      "Epoch: 1/5 | Loss: 8.2124\n",
      "Epoch: 1/5 | Loss: 7.7533\n",
      "Epoch: 1/5 | Loss: 8.4397\n",
      "Epoch: 1/5 | Loss: 7.1584\n",
      "Epoch: 1/5 | Loss: 7.3897\n",
      "Epoch: 1/5 | Loss: 7.1260\n",
      "Epoch: 1/5 | Loss: 7.2874\n",
      "Epoch: 1/5 | Loss: 7.6360\n",
      "Epoch: 1/5 | Loss: 7.8230\n",
      "Epoch: 1/5 | Loss: 7.7473\n",
      "Epoch: 1/5 | Loss: 7.8558\n",
      "Epoch: 1/5 | Loss: 8.4427\n",
      "Epoch: 1/5 | Loss: 6.9437\n",
      "Epoch: 1/5 | Loss: 7.5456\n",
      "Epoch: 1/5 | Loss: 8.1626\n",
      "Epoch: 1/5 | Loss: 7.7544\n",
      "Epoch: 1/5 | Loss: 8.8154\n",
      "Epoch: 1/5 | Loss: 7.7346\n",
      "Epoch: 1/5 | Loss: 8.1776\n",
      "Epoch: 1/5 | Loss: 8.8698\n",
      "Epoch: 1/5 | Loss: 8.4030\n",
      "Epoch: 1/5 | Loss: 8.7151\n",
      "Epoch: 1/5 | Loss: 8.3986\n",
      "Epoch: 1/5 | Loss: 8.6300\n",
      "Epoch: 1/5 | Loss: 7.8221\n",
      "Epoch: 1/5 | Loss: 8.3094\n",
      "Epoch: 1/5 | Loss: 7.8474\n",
      "Epoch: 1/5 | Loss: 7.8344\n",
      "Epoch: 1/5 | Loss: 8.0348\n",
      "Epoch: 1/5 | Loss: 7.9264\n",
      "Epoch: 1/5 | Loss: 7.7677\n",
      "Epoch: 1/5 | Loss: 8.1291\n",
      "Epoch: 1/5 | Loss: 8.2009\n",
      "Epoch: 1/5 | Loss: 8.4294\n",
      "Epoch: 1/5 | Loss: 7.6485\n",
      "Epoch: 1/5 | Loss: 8.4867\n",
      "Epoch: 1/5 | Loss: 7.9565\n",
      "Epoch: 1/5 | Loss: 7.7965\n",
      "Epoch: 1/5 | Loss: 8.4079\n",
      "Epoch: 1/5 | Loss: 8.4646\n",
      "Epoch: 1/5 | Loss: 7.7556\n",
      "Epoch: 1/5 | Loss: 8.2941\n",
      "Epoch: 1/5 | Loss: 8.3879\n",
      "Epoch: 1/5 | Loss: 7.7671\n",
      "Epoch: 1/5 | Loss: 8.0676\n",
      "Epoch: 1/5 | Loss: 7.6494\n",
      "Epoch: 1/5 | Loss: 8.5831\n",
      "Epoch: 1/5 | Loss: 7.6341\n",
      "Epoch: 1/5 | Loss: 6.6479\n",
      "Epoch: 1/5 | Loss: 7.6054\n",
      "Epoch: 1/5 | Loss: 7.7672\n",
      "Epoch: 1/5 | Loss: 8.1299\n",
      "Epoch: 1/5 | Loss: 7.8588\n",
      "Epoch: 1/5 | Loss: 7.8826\n",
      "Epoch: 1/5 | Loss: 7.3359\n",
      "Epoch: 1/5 | Loss: 7.4372\n",
      "Epoch: 1/5 | Loss: 7.8891\n",
      "Epoch: 1/5 | Loss: 7.8655\n",
      "Epoch: 1/5 | Loss: 7.4320\n",
      "Epoch: 1/5 | Loss: 8.3261\n",
      "Epoch: 1/5 | Loss: 7.8391\n",
      "Epoch: 1/5 | Loss: 7.4504\n",
      "Epoch: 1/5 | Loss: 7.6778\n",
      "Epoch: 1/5 | Loss: 7.8099\n",
      "Epoch: 1/5 | Loss: 7.4849\n",
      "Epoch: 1/5 | Loss: 7.4431\n",
      "Epoch: 1/5 | Loss: 8.6976\n",
      "Epoch: 1/5 | Loss: 7.9890\n",
      "Epoch: 1/5 | Loss: 8.3484\n",
      "Epoch: 1/5 | Loss: 7.1692\n",
      "Epoch: 1/5 | Loss: 7.5990\n",
      "Epoch: 1/5 | Loss: 8.0358\n",
      "Epoch: 1/5 | Loss: 7.9802\n",
      "Epoch: 1/5 | Loss: 7.6721\n",
      "Epoch: 1/5 | Loss: 7.8958\n",
      "Epoch: 1/5 | Loss: 7.5101\n",
      "Epoch: 1/5 | Loss: 8.3539\n",
      "Epoch: 1/5 | Loss: 7.9395\n",
      "Epoch: 1/5 | Loss: 8.3139\n",
      "Epoch: 1/5 | Loss: 8.8144\n",
      "Epoch: 1/5 | Loss: 8.3304\n",
      "Epoch: 1/5 | Loss: 7.1131\n",
      "Epoch: 1/5 | Loss: 7.5542\n",
      "Epoch: 1/5 | Loss: 7.2780\n",
      "Epoch: 1/5 | Loss: 7.8749\n",
      "Epoch: 1/5 | Loss: 6.8492\n",
      "Epoch: 1/5 | Loss: 7.9960\n",
      "Epoch: 1/5 | Loss: 7.9725\n",
      "Epoch: 1/5 | Loss: 7.5117\n",
      "Epoch: 1/5 | Loss: 7.7758\n",
      "Epoch: 1/5 | Loss: 7.7145\n",
      "Epoch: 1/5 | Loss: 7.7350\n",
      "Epoch: 1/5 | Loss: 8.6752\n",
      "Epoch: 1/5 | Loss: 7.3447\n",
      "Epoch: 1/5 | Loss: 7.7332\n",
      "Epoch: 1/5 | Loss: 7.0950\n",
      "Epoch: 1/5 | Loss: 8.1595\n",
      "Epoch: 1/5 | Loss: 8.1969\n",
      "Epoch: 1/5 | Loss: 8.2669\n",
      "Epoch: 1/5 | Loss: 8.7207\n",
      "Epoch: 1/5 | Loss: 7.8628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 | Loss: 7.8866\n",
      "Epoch: 1/5 | Loss: 9.0361\n",
      "Epoch: 1/5 | Loss: 7.2404\n",
      "Epoch: 1/5 | Loss: 8.7436\n",
      "Epoch: 1/5 | Loss: 8.3635\n",
      "Epoch: 1/5 | Loss: 8.1681\n",
      "Epoch: 1/5 | Loss: 7.2061\n",
      "Epoch: 1/5 | Loss: 7.3893\n",
      "Epoch: 1/5 | Loss: 7.7692\n",
      "Epoch: 1/5 | Loss: 8.0029\n",
      "Epoch: 1/5 | Loss: 7.6422\n",
      "Epoch: 1/5 | Loss: 8.5139\n",
      "Epoch: 1/5 | Loss: 7.9026\n",
      "Epoch: 1/5 | Loss: 7.8650\n",
      "Epoch: 1/5 | Loss: 7.2219\n",
      "Epoch: 1/5 | Loss: 8.1737\n",
      "Epoch: 1/5 | Loss: 8.3136\n",
      "Epoch: 1/5 | Loss: 7.7672\n",
      "Epoch: 1/5 | Loss: 7.9126\n",
      "Epoch: 1/5 | Loss: 8.0534\n",
      "Epoch: 1/5 | Loss: 7.6269\n",
      "Epoch: 1/5 | Loss: 8.4140\n",
      "Epoch: 1/5 | Loss: 8.2485\n",
      "Epoch: 1/5 | Loss: 7.4338\n",
      "Epoch: 1/5 | Loss: 7.6831\n",
      "Epoch: 1/5 | Loss: 7.8083\n",
      "Epoch: 1/5 | Loss: 7.8660\n",
      "Epoch: 1/5 | Loss: 7.9943\n",
      "Epoch: 1/5 | Loss: 7.6100\n",
      "Epoch: 1/5 | Loss: 8.0182\n",
      "Epoch: 1/5 | Loss: 7.8958\n",
      "Epoch: 1/5 | Loss: 8.1396\n",
      "Epoch: 1/5 | Loss: 8.1852\n",
      "Epoch: 1/5 | Loss: 7.6199\n",
      "Epoch: 1/5 | Loss: 8.3657\n",
      "Epoch: 1/5 | Loss: 8.6641\n",
      "Epoch: 1/5 | Loss: 8.6109\n",
      "Epoch: 1/5 | Loss: 7.2257\n",
      "Epoch: 1/5 | Loss: 7.5994\n",
      "Epoch: 1/5 | Loss: 7.6912\n",
      "Epoch: 1/5 | Loss: 7.7517\n",
      "Epoch: 1/5 | Loss: 7.7021\n",
      "Epoch: 1/5 | Loss: 8.1272\n",
      "Epoch: 1/5 | Loss: 8.5890\n",
      "Epoch: 1/5 | Loss: 8.3062\n",
      "Epoch: 1/5 | Loss: 8.2871\n",
      "Epoch: 1/5 | Loss: 8.2743\n",
      "Epoch: 1/5 | Loss: 7.6284\n",
      "Epoch: 1/5 | Loss: 6.0415\n",
      "Epoch: 1/5 | Loss: 8.2919\n",
      "Epoch: 1/5 | Loss: 5.5046\n",
      "Epoch: 1/5 | Loss: 5.9598\n",
      "Epoch: 1/5 | Loss: 8.2205\n",
      "Epoch: 1/5 | Loss: 7.2470\n",
      "Epoch: 1/5 | Loss: 7.4232\n",
      "Epoch: 1/5 | Loss: 5.3707\n",
      "Epoch: 1/5 | Loss: 5.7161\n",
      "Epoch: 1/5 | Loss: 6.9394\n",
      "Epoch: 1/5 | Loss: 6.1245\n",
      "Epoch: 1/5 | Loss: 7.4800\n",
      "Epoch: 1/5 | Loss: 7.8899\n",
      "Epoch: 1/5 | Loss: 7.9724\n",
      "Epoch: 1/5 | Loss: 7.2160\n",
      "Epoch: 1/5 | Loss: 6.5801\n",
      "Epoch: 1/5 | Loss: 7.2509\n",
      "Epoch: 1/5 | Loss: 7.0148\n",
      "Epoch: 1/5 | Loss: 6.8718\n",
      "Epoch: 1/5 | Loss: 5.0948\n",
      "Epoch: 1/5 | Loss: 4.5884\n",
      "Epoch: 1/5 | Loss: 7.7366\n",
      "Epoch: 1/5 | Loss: 7.2771\n",
      "Epoch: 1/5 | Loss: 7.1267\n",
      "Epoch: 1/5 | Loss: 8.1261\n",
      "Epoch: 1/5 | Loss: 7.8576\n",
      "Epoch: 1/5 | Loss: 9.2940\n",
      "Epoch: 1/5 | Loss: 6.8696\n",
      "Epoch: 1/5 | Loss: 7.6011\n"
     ]
    }
   ],
   "source": [
    "train_cbow(    w1,\n",
    "               w2,\n",
    "               int_words,\n",
    "               n_vocab=n_vocab,\n",
    "               n_embed=n_embed,\n",
    "               learning_rate=lr,\n",
    "               batch_size=batch_size,\n",
    "               n_epochs=n_epochs,\n",
    "               print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac96ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final embeddings is the summation of the two matrix (check lecture slides)\n",
    "embeddings = w1.data.to('cpu').data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d8a91",
   "metadata": {},
   "source": [
    "### Evaluation via Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c995ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_viz_words = 200\n",
    "tsne = TSNE()\n",
    "embeddings_tsne = tsne.fit_transform(embeddings[:n_viz_words, :])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (20, 20))\n",
    "for i in range(n_viz_words):\n",
    "    plt.scatter(*embeddings_tsne[i, :], color = 'red', s=40)\n",
    "    plt.annotate(i2w[i], (embeddings_tsne[i, 0], embeddings_tsne[i, 1]), alpha = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0a899",
   "metadata": {},
   "source": [
    "### Evaluation via Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(query, w2i, embeddings, strategy):\n",
    "    # TODO\n",
    "    # input: query or document, vocabulary w2i\n",
    "    # input: trained word2vec embeddings\n",
    "    # input: strategy: either 'average' or 'concatenate'\n",
    "    # output: vector representation of the document query\n",
    "    w2v_query = tokenize_doc(query, lemma=use_lemmatization, remove_stopwords=remove_stopwords)\n",
    "    idx = [w2i[word] for word in w2v_query]\n",
    "    v = [embeddings[i] for i in idx]\n",
    "    \n",
    "    assert strategy in ['average', 'concatenate']\n",
    "    \n",
    "    # get vectors of each word in the query\n",
    "    \n",
    "    # sentence aggregation strategy\n",
    "    if strategy == 'average':\n",
    "        # TODO START\n",
    "        vector = [em.mean() for em in v]\n",
    "        # vector = \n",
    "        # TODO END\n",
    "    else:\n",
    "        # TODO START\n",
    "        vector = np.ravel(v)\n",
    "        # TODO END\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04afb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test document similarity\n",
    "q = transform('today I am very happy', w2i, embeddings, strategy='average')\n",
    "v = transform('today I feel so fascinated', w2i, embeddings, strategy='average')\n",
    "sim = np.dot(q, v)/(np.linalg.norm(q)* np.linalg.norm(v))\n",
    "print(\"Cosine Similarity: {}\".format(sim)) # this score should be high / close to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(strategy):\n",
    "    df = pd.read_csv('./quora_train.csv')\n",
    "    print(\"Loaded {} pairs\".format(len(df)))\n",
    "    pairs = list(zip(df['question1'].astype(str), df['question2'].astype(str)))\n",
    "    \n",
    "    all_sims = []\n",
    "    \n",
    "    for doc1, doc2 in tqdm(pairs):\n",
    "        q = transform(doc1, w2i, embeddings, strategy=strategy)\n",
    "        v = transform(doc2, w2i, embeddings, strategy=strategy)\n",
    "        \n",
    "        diff = len(q) - len(v)\n",
    "        if diff > 0:\n",
    "            v = np.pad(v, (0, np.abs(diff)))\n",
    "        else:\n",
    "            q = np.pad(q, (0, np.abs(diff)))\n",
    "        \n",
    "        sim = np.dot(q, v)/(np.linalg.norm(q)* np.linalg.norm(v))\n",
    "        all_sims.append(sim)\n",
    "        \n",
    "    return np.mean(all_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf214d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_similarity = evaluation('average')\n",
    "print(\"Final Average Similarity using Average Strategy: {}\".format(avg_similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_similarity = evaluation('concatenate')\n",
    "print(\"Final Average Similarity using Concatenation Strategy: {}\".format(avg_similarity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0718306951c543f9ab1700229e5c3e5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9dbd4eaf2f26468a91a88fd22327460f",
       "style": "IPY_MODEL_375bd16927644612842cf2215d231b6b",
       "value": " 1/1 [00:00&lt;00:00,  1.83it/s]"
      }
     },
     "29e273c83d6d4107849fb5ac592ed40c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2e6d280046ae4371bc8e4f3519c4c142": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "375bd16927644612842cf2215d231b6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "539046ed413540b8929a07b234771d6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8d857fdcc2e3494b94923334716c0336",
       "style": "IPY_MODEL_2e6d280046ae4371bc8e4f3519c4c142",
       "value": "100%"
      }
     },
     "5b08beeb860c43328729559b1394f2d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7b3f683b32eb4a6791ebc0283dcd524a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_539046ed413540b8929a07b234771d6b",
        "IPY_MODEL_99e74ce0b94a497f971d09f794564e6f",
        "IPY_MODEL_0718306951c543f9ab1700229e5c3e5d"
       ],
       "layout": "IPY_MODEL_5b08beeb860c43328729559b1394f2d8"
      }
     },
     "8d857fdcc2e3494b94923334716c0336": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "99e74ce0b94a497f971d09f794564e6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_cab5c9b4fd08470cb51b1bd2dbda7e23",
       "max": 1,
       "style": "IPY_MODEL_29e273c83d6d4107849fb5ac592ed40c",
       "value": 1
      }
     },
     "9dbd4eaf2f26468a91a88fd22327460f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cab5c9b4fd08470cb51b1bd2dbda7e23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
