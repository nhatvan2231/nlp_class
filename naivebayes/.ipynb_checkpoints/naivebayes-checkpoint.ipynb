{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f1d95443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f9de47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clickbait_data():\n",
    "    df = pd.read_csv('./clickbait_data.csv')\n",
    "    df, test_df = train_test_split(df, test_size=0.1, random_state=17)\n",
    "    texts = df['headline']\n",
    "    labels = df['clickbait'].values.astype(int)\n",
    "    test_texts = test_df['headline'].values\n",
    "    test_labels = test_df['clickbait'].values.astype(int)\n",
    "    \n",
    "    return texts, labels, test_texts, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "aa977f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(texts, test_texts):\n",
    "    vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "    \n",
    "    return X, X_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "144fd0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(X, labels, vectorizer):\n",
    "    X_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    X_df['--label--'] = labels.astype(int)\n",
    "    \n",
    "    # log_prob_class\n",
    "    class_count_ = X_df['--label--'].value_counts()\n",
    "    log_class_count = np.log(class_count_)\n",
    "    log_prob_class = log_class_count - np.log(class_count_.sum())\n",
    "    \n",
    "    # log_prob_token\n",
    "    token_count_per_class = X_df.groupby('--label--').sum().reset_index(level=0, drop=True)\n",
    "    token_count_per_class = token_count_per_class + 1.0\n",
    "    token_per_class = token_count_per_class.sum(1).values.reshape(-1, 1)\n",
    "    \n",
    "    prob_token_count_per_class = np.log(token_count_per_class) - np.log(token_per_class)\n",
    "    \n",
    "    return prob_token_count_per_class, log_prob_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "defed45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_doc(doc, vectorizer, prob_token_count_per_class, log_prob_class):\n",
    "    # TODO\n",
    "    # This function input a document ``doc\" and return the prediction ``pred\" (numeric output of 0 or 1)\n",
    "    # An output ``pred\" of 0 means we predict the document ``doc\" as non-clickbait\n",
    "    # An output ``pred\"of 1 means we predict the document ``doc\" as clickbait\n",
    "    c_sum = np.array([0.0,0.0])\n",
    "    c_sum += np.sum([prob_token_count_per_class[token]\\\n",
    "            for token in doc.split() if token in vectorizer.get_feature_names_out()],0)\n",
    "    nonclickbait = c_sum[0]+log_prob_class[0]\n",
    "    clickbait = c_sum[1]+log_prob_class[1]\n",
    "    pred = np.argmax([nonclickbait, clickbait])\n",
    "    # END TODO\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75359d71",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d9789ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels, test_texts, test_labels = load_clickbait_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "38b23706",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, vectorizer = feature_extraction(texts, test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e61aefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_token_count_per_class, log_prob_class = train_naive_bayes(X, labels, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4ddd2d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6952326870509662 -0.6910660143561209\n",
      "Done 0\n",
      "-17.14661481429669 -23.458703125520923\n",
      "-43.889863874317705 -51.80201630228768\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-6.554229701205301 -12.074884569938522\n",
      "-20.317699975328544 -20.819645795905664\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-21.709512546468687 -30.485812854413734\n",
      "-25.16646361971268 -32.645297103767106\n",
      "-8.917439416015782 -5.822980686772634\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-32.13236355885125 -40.650391133539415\n",
      "-17.938925743976952 -22.360090836852816\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-39.30585549555108 -46.38651668111666\n",
      "-16.03296464826404 -22.360090836852816\n",
      "-21.194174310745233 -30.748177118881223\n",
      "-8.694295864701571 -6.975018142114323\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-23.30228340641758 -30.748177118881227\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-35.57080731936297 -41.256526937109726\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-9.151054267197287 -6.594245646596531\n",
      "-9.322904524123945 -7.254603004333485\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-59.361963856421546 -57.02961963922\n",
      "-9.387443045261517 -10.128974420883209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-17.0374155223317 -20.462970851966933\n",
      "-9.792908153369682 -6.260754038113456\n",
      "-25.825625953440515 -31.664467850755376\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-8.763288736188523 -5.694762033038757\n",
      "-59.842908536008984 -68.18359343047325\n",
      "-16.210932893107888 -19.29982004216125\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-16.60007110792862 -20.97379647573292\n",
      "-31.562020645502514 -39.20525627239658\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-58.40667848964608 -73.66971181957987\n",
      "-20.017432343942623 -29.55425465040879\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-8.266851849874632 -11.381737389378577\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-8.763288736188523 -5.694762033038757\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-22.897334960682052 -31.58442514308184\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-31.925500893141177 -41.256526937109726\n",
      "-8.763288736188523 -5.694762033038757\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-8.484575333719503 -7.037931967524893\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-34.903214661192486 -37.79252865420854\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-9.205121488467563 -6.756764576094306\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-26.98176208624681 -31.664467850755383\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-8.334293130670165 -6.284924399041269\n",
      "-17.491866738569946 -23.458703125520923\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-8.43193160023408 -7.9317498435469895\n",
      "-8.917439416015782 -5.822980686772634\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-7.861386741766467 -10.688590208818631\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-18.365855090753413 -20.462970851966933\n",
      "-8.763288736188523 -5.694762033038757\n",
      "-23.028496122284615 -29.7673478658695\n",
      "-9.456435916748468 -11.381737389378577\n",
      "-15.440596073856808 -20.819645795905664\n",
      "-25.504186116967688 -33.45622731998344\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-30.294713089599647 -35.26854065437807\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-8.917439416015782 -5.822980686772634\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-8.052441978529178 -7.230697483479931\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-18.27000713196329 -21.156118032526877\n",
      "-33.52343498109379 -41.43884849390368\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-8.629757343564 -5.651637606405003\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-8.334293130670165 -6.284924399041269\n",
      "-36.80467304767697 -42.314317231257576\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-19.219087686660437 -12.176361320962599\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-9.53054388890219 -6.838442607108573\n",
      "-8.917439416015782 -5.822980686772634\n",
      "-36.075934922014326 -39.680990576351306\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-0.6952326870509662 -0.6910660143561209\n",
      "-8.728197416377252 -12.074884569938522\n",
      "-23.311643039571003 -30.237351495115234\n",
      "-24.468745477526916 -29.87270838152732\n",
      "-14.896612031613083 -20.367660672162607\n",
      "-17.652209388645126 -22.072408764401032\n",
      "-0.6952326870509662 -0.6910660143561209\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5252/2805177570.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_single_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_token_count_per_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5252/2965086006.py\u001b[0m in \u001b[0;36mpredict_single_doc\u001b[0;34m(doc, vectorizer, prob_token_count_per_class, log_prob_class)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# An output ``pred\"of 1 means we predict the document ``doc\" as clickbait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mc_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     c_sum += np.sum([prob_token_count_per_class[token]\\\n\u001b[0m\u001b[1;32m      8\u001b[0m             for token in doc.split() if token in vectorizer.get_feature_names_out()],0)\n\u001b[1;32m      9\u001b[0m     \u001b[0mnonclickbait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlog_prob_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5252/2965086006.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mc_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     c_sum += np.sum([prob_token_count_per_class[token]\\\n\u001b[0;32m----> 8\u001b[0;31m             for token in doc.split() if token in vectorizer.get_feature_names_out()],0)\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mnonclickbait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlog_prob_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mclickbait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlog_prob_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mget_feature_names_out\u001b[0;34m(self, input_features)\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         return np.asarray(\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for i, doc in enumerate(test_texts):\n",
    "    preds.append(predict_single_doc(doc, vectorizer, prob_token_count_per_class, log_prob_class))\n",
    "    if i % 500 == 0:\n",
    "        print(\"Done\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "59dedced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.75      1631\n",
      "           1       0.71      0.99      0.83      1569\n",
      "\n",
      "    accuracy                           0.80      3200\n",
      "   macro avg       0.85      0.80      0.79      3200\n",
      "weighted avg       0.85      0.80      0.79      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94b413",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdfc07f",
   "metadata": {},
   "source": [
    "#### Compare your results with the scikit-learn Naive Bayes implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "93e0bbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      1631\n",
      "           1       0.95      0.97      0.96      1569\n",
      "\n",
      "    accuracy                           0.96      3200\n",
      "   macro avg       0.96      0.96      0.96      3200\n",
      "weighted avg       0.96      0.96      0.96      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, labels)\n",
    "print(classification_report(test_labels, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac1cd90",
   "metadata": {},
   "source": [
    "##### Make sure the results from your implementation is the same or similar to the one implemented in scikit-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
