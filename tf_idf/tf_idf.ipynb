{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeac22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you cannot load any other libaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tqdm\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f202807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nytimes_data_final.csv')\n",
    "df = df.drop_duplicates('text')\n",
    "N = len(df)\n",
    "corpus = df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e19c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords = True\n",
    "use_lemmatization = False\n",
    "l2_normalize_tf_idf = False\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fdbc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(q, v):\n",
    "    sim = np.dot(q, v)/(np.linalg.norm(q)* np.linalg.norm(v))\n",
    "    \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d966b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc(sent, lemma=False, remove_stopwords=False):\n",
    "    # a simple tokenizer with case folding and an option to use lemmatization or remove stopwords\n",
    "    sent = sent.lower()\n",
    "    tokens = sent.split()\n",
    "    if lemma:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8164847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_text_processing(corpus):\n",
    "    # This function will go through the corpus and outputs two components\n",
    "    # w2i: the mapping of a vocabular to in index. This is also our vocabulary\n",
    "    # docs_in_tokens: list of extracted tokens for each document\n",
    "    vocab = set()\n",
    "    docs_in_tokens = []\n",
    "    for doc in corpus:\n",
    "        tokens = tokenize_doc(doc, lemma=use_lemmatization, remove_stopwords=remove_stopwords)\n",
    "        vocab.update(set(tokens))\n",
    "        docs_in_tokens.append(tokens)\n",
    "    vocab = list(vocab)\n",
    "    vocab.sort()\n",
    "    w2i = OrderedDict()\n",
    "    for i, word in enumerate(vocab):\n",
    "        w2i[word] = i\n",
    "    \n",
    "    return w2i, docs_in_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc7efce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(docs_in_tokens, w2i):\n",
    "    # TASK: given the list of tokens for each document (docs_in_tokens) and the vocabulary (w2i),\n",
    "    # you are asked to calculate the inverse document frequency (IDF) of each word using the formulation\n",
    "    # log10(N/(df+1))\n",
    "    # RETURN: all_idf vector (or a column) contains all the IDF of all words in the vocabulary\n",
    "    \n",
    "    all_idf = [] #initialize the list of all idf values\n",
    "    \n",
    "    # TODO:\n",
    "    N = len(docs_in_tokens)\n",
    "    w_fre = [Counter(doc) for doc in docs_in_tokens]\n",
    "    for w in w2i:\n",
    "        idf = 0\n",
    "        for i,doc in enumerate(docs_in_tokens):\n",
    "            if w_fre[i][w] > 0:\n",
    "                idf += 1\n",
    "        idf = np.log10(idf)\n",
    "        all_idf.append(idf)\n",
    "    \n",
    "    return np.array(all_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "178bbc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(docs_in_tokens, w2i):\n",
    "    # TASK: given the list of tokens for each document (docs_in_tokens) and the vocabulary (w2i),\n",
    "    # you are asked to calculate the term-frequency table or matrix using the formulation:\n",
    "    # tf = log10(frequency+1)\n",
    "    # RETURN: tf_matrix as the term-frequency table\n",
    "    \n",
    "    tf_matrix = np.zeros((len(w2i), len(docs_in_tokens))) #initialize as a matrix/table of all zeros\n",
    "    \n",
    "    # TODO\n",
    "    N = len(docs_in_tokens)\n",
    "    w_fre = [Counter(doc) for doc in docs_in_tokens]\n",
    "    for w in w2i:\n",
    "        for i,doc in enumerate(docs_in_tokens):\n",
    "            tf = w_fre[i][w]\n",
    "            tf_matrix[w2i[w],i] = np.log10(tf+1)\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5e8e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(query, w2i, all_idf):\n",
    "    # TASK: given a string query, you are asked to utilize the extracted vocabulary (w2i)\n",
    "    # and idf value for each word to transform a query into a respective tf-idf vector\n",
    "    # RETURN: tf_idf_query\n",
    "    \n",
    "    tf_idf_query = []\n",
    "    \n",
    "    #TODO\n",
    "    q = tokenize_doc(query,False,True)\n",
    "    for i in q:\n",
    "        tf_idf_query.append(all_idf[w2i[i]])\n",
    "    return np.array(tf_idf_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ef62312",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i, docs_in_tokens = basic_text_processing(corpus)\n",
    "assert len(docs_in_tokens) == len(corpus) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f3b8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_idf = calculate_idf(docs_in_tokens, w2i)\n",
    "assert len(all_idf) == len(w2i) \n",
    "# if you have error in this, please check your calculate_idf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82b8da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix = calculate_tf(docs_in_tokens, w2i)\n",
    "assert tf_matrix.shape == (len(w2i), len(docs_in_tokens))\n",
    "# if you have error in this, please check your calculate_tf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f4cb821",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tf_matrix * all_idf.reshape(-1,1) # final tf-idf is the multiplicatioin of tf and idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cec1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if l2_normalize_tf_idf:\n",
    "    from sklearn.preprocessing import normalize\n",
    "    tf_idf = normalize(tf_idf, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86089e70",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b44b9",
   "metadata": {},
   "source": [
    "## Evaluation via Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "654521b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, k):\n",
    "    q = transform(query, w2i, all_idf)\n",
    "    sims = []\n",
    "    for i in range(tf_idf.shape[1]):\n",
    "        v = tf_idf[:,i].reshape(-1,)\n",
    "        sim = np.dot(q, v)/(np.linalg.norm(q)* np.linalg.norm(v))\n",
    "        sims.append(sim)\n",
    "    idx = np.argsort(sims)[::-1]\n",
    "    return idx[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d988f",
   "metadata": {},
   "source": [
    "#### Let's try to search a document from the corpus via a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f294e8ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (7813,) not aligned: 2 (dim 0) != 7813 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5039/2109110614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Trump and Biden\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfound_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfound_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5039/2695025671.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(query, k)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0msims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,) and (7813,) not aligned: 2 (dim 0) != 7813 (dim 0)"
     ]
    }
   ],
   "source": [
    "query = \"Trump and Biden\"\n",
    "found_idx = search(query, 10)\n",
    "corpus[found_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "395b70e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (7813,) not aligned: 2 (dim 0) != 7813 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5039/2584517180.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,) and (7813,) not aligned: 2 (dim 0) != 7813 (dim 0)"
     ]
    }
   ],
   "source": [
    "tf_idf_query = []\n",
    "#TODO\n",
    "q = tokenize_doc(query,False,True)\n",
    "for i in q:\n",
    "    tf_idf_query.append(all_idf[w2i[i]])\n",
    "q = tf_idf_query\n",
    "for i in range(tf_idf.shape[1]):\n",
    "    v = tf_idf[:,i].reshape(-1,)\n",
    "    sim = np.dot(q, v)/(np.linalg.norm(q)* np.linalg.norm(v))\n",
    "    sims.append(sim)\n",
    "idx = np.argsort(sims)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcabb23f",
   "metadata": {},
   "source": [
    "#### Let's test your TF-IDF on an information retrieval task to see if the results match with when using Scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = {'Trump and Biden': [598, 2299, 595, 2968, 775, 1123, 2953, 1220, 2346, 853], 'Trump Twitter': [598, 2649, 292, 1102, 2308, 196, 1315, 1283, 1034, 1012], 'Elon Musk Trump': [598, 1273, 1656, 1823, 146, 1306, 81, 127, 1188, 1664], 'Political Conflicts': [598, 964, 1598, 621, 2219, 2640, 2377, 455, 1959, 2537], 'University of Misississippi': [598, 2497, 2171, 2744, 682, 1620, 3032, 1007, 1013, 1012], 'Thai Le': [598, 401, 2721, 3032, 1008, 1015, 1014, 1013, 1012, 1011], 'covid-19 is very dangerous': [598, 2736, 521, 1712, 821, 1625, 948, 2835, 168, 253], 'Defense Secretary Will Assess How to Promote More Minorities in Military': [598, 2235, 2557, 2546, 395, 1649, 716, 152, 2195, 1441], 'When Luxury Stores Decorate Their Riot Barricades With Protest Art': [598, 2465, 382, 132, 2392, 2339, 203, 0, 1142, 212]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f8b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_recall = []\n",
    "for query in test_set:\n",
    "    true_answers = set(test_set[query])\n",
    "    found_idx = set(search(query, 10))\n",
    "    recall = len(found_idx.intersection(true_answers))/len(true_answers)\n",
    "    avg_recall.append(recall)\n",
    "    print(\"'{}'\".format(query), \"recall =\", recall)\n",
    "mean_recall = np.mean(avg_recall)\n",
    "print(\"Average Recall\", mean_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a9704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
