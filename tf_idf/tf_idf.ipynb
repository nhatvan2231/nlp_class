{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aeac22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you cannot load any other libaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tqdm\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2f202807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nytimes_data_final.csv')\n",
    "df = df.drop_duplicates('text')\n",
    "N = len(df)\n",
    "corpus = df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0e19c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords = False\n",
    "use_lemmatization = False\n",
    "l2_normalize_tf_idf = False\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6fdbc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(q, v):\n",
    "    sim = np.dot(q, v)/(np.linalg.norm(q)* np.linalg.norm(v))\n",
    "    \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2d966b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc(sent, lemma=False, remove_stopwords=False):\n",
    "    # a simple tokenizer with case folding and an option to use lemmatization or remove stopwords\n",
    "    sent = sent.lower()\n",
    "    tokens = sent.split()\n",
    "    if lemma:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8164847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_text_processing(corpus):\n",
    "    # This function will go through the corpus and outputs two components\n",
    "    # w2i: the mapping of a vocabular to in index. This is also our vocabulary\n",
    "    # docs_in_tokens: list of extracted tokens for each document\n",
    "    vocab = set()\n",
    "    docs_in_tokens = []\n",
    "    for doc in corpus:\n",
    "        tokens = tokenize_doc(doc, lemma = use_lemmatization, remove_stopwords=remove_stopwords)\n",
    "        vocab.update(set(tokens))\n",
    "        docs_in_tokens.append(tokens)\n",
    "    vocab = list(vocab)\n",
    "    vocab.sort()\n",
    "    w2i = OrderedDict()\n",
    "    for i, word in enumerate(vocab):\n",
    "        w2i[word] = i\n",
    "    \n",
    "    return w2i, docs_in_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bc7efce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(docs_in_tokens, w2i):\n",
    "    # TASK: given the list of tokens for each document (docs_in_tokens) and the vocabulary (w2i),\n",
    "    # you are asked to calculate the inverse document frequency (IDF) of each word using the formulation\n",
    "    # log10(N/(df+1))\n",
    "    # RETURN: all_idf vector (or a column) contains all the IDF of all words in the vocabulary\n",
    "    \n",
    "    all_idf = [] #initialize the list of all idf values\n",
    "    \n",
    "    # TODO:\n",
    "    N = len(docs_in_tokens)\n",
    "    w_fre = [Counter(doc) for doc in docs_in_tokens]\n",
    "    for w in w2i:\n",
    "        idf = 0\n",
    "        for i,doc in enumerate(docs_in_tokens):\n",
    "            if w_fre[i][w] > 0:\n",
    "                idf += 1\n",
    "        idf = np.log10(idf)\n",
    "        all_idf.append(idf)\n",
    "    \n",
    "    return np.array(all_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "178bbc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(docs_in_tokens, w2i):\n",
    "    # TASK: given the list of tokens for each document (docs_in_tokens) and the vocabulary (w2i),\n",
    "    # you are asked to calculate the term-frequency table or matrix using the formulation:\n",
    "    # tf = log10(frequency+1)\n",
    "    # RETURN: tf_matrix as the term-frequency table\n",
    "    \n",
    "    tf_matrix = np.zeros((len(w2i), len(docs_in_tokens))) #initialize as a matrix/table of all zeros\n",
    "    \n",
    "    # TODO\n",
    "    N = len(docs_in_tokens)\n",
    "    w_fre = [Counter(doc) for doc in docs_in_tokens]\n",
    "    for w in w2i:\n",
    "        for i,doc in enumerate(docs_in_tokens):\n",
    "            tf = w_fre[i][w]\n",
    "            tf_matrix[w2i[w],i] = np.log10(tf+1)\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b5e8e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(query, w2i, all_idf):\n",
    "    # TASK: given a string query, you are asked to utilize the extracted vocabulary (w2i)\n",
    "    # and idf value for each word to transform a query into a respective tf-idf vector\n",
    "    # RETURN: tf_idf_query\n",
    "    \n",
    "    tf_idf_query = []\n",
    "    \n",
    "    #TODO\n",
    "    q = [tokenize_doc(query, True, remove_stopwords=False)]\n",
    "    tf_query = calculate_tf(q, w2i)\n",
    "    #idf_query = calculate_idf(query, w2i)\n",
    "    tf_idf_query = (tf_query*all_idf.reshape(-1,1)).reshape(-1,)\n",
    "    #tf_idf_query = tf_idf_query.reshape(-1,)\n",
    "    return np.array(tf_idf_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1ef62312",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i, docs_in_tokens = basic_text_processing(corpus)\n",
    "assert len(docs_in_tokens) == len(corpus) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2f3b8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_idf = calculate_idf(docs_in_tokens, w2i)\n",
    "assert len(all_idf) == len(w2i) \n",
    "# if you have error in this, please check your calculate_idf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "82b8da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix = calculate_tf(docs_in_tokens, w2i)\n",
    "assert tf_matrix.shape == (len(w2i), len(docs_in_tokens))\n",
    "# if you have error in this, please check your calculate_tf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9f4cb821",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tf_matrix * all_idf.reshape(-1,1) # final tf-idf is the multiplicatioin of tf and idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4cec1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if l2_normalize_tf_idf:\n",
    "    from sklearn.preprocessing import normalize\n",
    "    tf_idf = normalize(tf_idf, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86089e70",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b44b9",
   "metadata": {},
   "source": [
    "## Evaluation via Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "654521b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, k):\n",
    "    q = transform(query, w2i, all_idf)\n",
    "    sims = []\n",
    "    for i in range(tf_idf.shape[1]):\n",
    "        v = tf_idf[:,i].reshape(-1,)\n",
    "        sim = np.dot(q, v)/(np.linalg.norm(q)* np.linalg.norm(v))\n",
    "        sims.append(sim)\n",
    "    idx = np.argsort(sims)[::-1]\n",
    "    return idx[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d988f",
   "metadata": {},
   "source": [
    "#### Let's try to search a document from the corpus via a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f294e8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8386/2695025671.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim = np.dot(q, v)/(np.linalg.norm(q)* np.linalg.norm(v))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Together.', 'Sincere', 'Airy Snack Item', '‘Weather’',\n",
       "       'Trump Speaks! And Speaks. And Speaks …',\n",
       "       'Trump and Transgender Rights',\n",
       "       'Trump Thinks He’s 2020’s ‘Law and Order’ Candidate. He’s Not.',\n",
       "       'TikTok Teens and K-Pop Stans Say They Sank Trump Rally',\n",
       "       'On North Korea and Iran, Bolton Blames ‘the Split Between Trump and Trump’',\n",
       "       'Switching Letters, Skipping Lines: Troubled and Dyslexic Minds'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Trump and Biden\"\n",
    "found_idx = search(query, 10)\n",
    "corpus[found_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cd62b99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1555,  598,   55, 1661, 2910, 1656,  221, 2354, 2338, 1725])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcabb23f",
   "metadata": {},
   "source": [
    "#### Let's test your TF-IDF on an information retrieval task to see if the results match with when using Scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d8d7d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = {'Trump and Biden': [598, 2299, 595, 2968, 775, 1123, 2953, 1220, 2346, 853], 'Trump Twitter': [598, 2649, 292, 1102, 2308, 196, 1315, 1283, 1034, 1012], 'Elon Musk Trump': [598, 1273, 1656, 1823, 146, 1306, 81, 127, 1188, 1664], 'Political Conflicts': [598, 964, 1598, 621, 2219, 2640, 2377, 455, 1959, 2537], 'University of Misississippi': [598, 2497, 2171, 2744, 682, 1620, 3032, 1007, 1013, 1012], 'Thai Le': [598, 401, 2721, 3032, 1008, 1015, 1014, 1013, 1012, 1011], 'covid-19 is very dangerous': [598, 2736, 521, 1712, 821, 1625, 948, 2835, 168, 253], 'Defense Secretary Will Assess How to Promote More Minorities in Military': [598, 2235, 2557, 2546, 395, 1649, 716, 152, 2195, 1441], 'When Luxury Stores Decorate Their Riot Barricades With Protest Art': [598, 2465, 382, 132, 2392, 2339, 203, 0, 1142, 212]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "46a966c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump and Biden\n",
      "Trump Twitter\n",
      "Elon Musk Trump\n",
      "Political Conflicts\n",
      "University of Misississippi\n",
      "Thai Le\n",
      "covid-19 is very dangerous\n",
      "Defense Secretary Will Assess How to Promote More Minorities in Military\n",
      "When Luxury Stores Decorate Their Riot Barricades With Protest Art\n"
     ]
    }
   ],
   "source": [
    "for query in test_set:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9d0f8b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8386/2695025671.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim = np.dot(q, v)/(np.linalg.norm(q)* np.linalg.norm(v))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Trump and Biden' recall = 0.1\n",
      "'Trump Twitter' recall = 0.2\n",
      "'Elon Musk Trump' recall = 0.2\n",
      "'Political Conflicts' recall = 0.7\n",
      "'University of Misississippi' recall = 0.1\n",
      "'Thai Le' recall = 0.6\n",
      "'covid-19 is very dangerous' recall = 0.1\n",
      "'Defense Secretary Will Assess How to Promote More Minorities in Military' recall = 0.2\n",
      "'When Luxury Stores Decorate Their Riot Barricades With Protest Art' recall = 0.2\n",
      "Average Recall 0.2666666666666667\n"
     ]
    }
   ],
   "source": [
    "avg_recall = []\n",
    "for query in test_set:\n",
    "    true_answers = set(test_set[query])\n",
    "    found_idx = set(search(query, 10))\n",
    "    recall = len(found_idx.intersection(true_answers))/len(true_answers)\n",
    "    avg_recall.append(recall)\n",
    "    print(\"'{}'\".format(query), \"recall =\", recall)\n",
    "mean_recall = np.mean(avg_recall)\n",
    "print(\"Average Recall\", mean_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
